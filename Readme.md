# Natural Language Processing
This is the Natural Language Processing repository of IvLabs and contains implementation of various architectures, starting from "Character Level RNN(s)" built from scratch, up to and including the almighty "Transformer" architecture.
Further, we have also included a rough roadmap for enthusiasts with basic knowledge of Machine/Deep Learning.

We have implemented and compared the following architectures:
- [ ] Character Level RNN (Char RNN)
    - [x] From scratch
    - [ ] Vanilla RNN
    - [ ] LSTM
    - [ ] GRU
- [ ] Language Models
    - [ ] Vanilla RNN
    - [ ] LSTM
    - [ ] GRU
- [ ] Neural Machine Translation
    - [ ] Vanilla Seq2Seq (using LSTM)
    - [ ] Seq2Seq with Attention (using GRU)
    - [ ] Convolutional Seq2Seq
    - [ ] Tansformer Network

We have also implemented the following papers:
- [ ] [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122)
- [ ] [Attention Is All You Need (modified)](https://arxiv.org/abs/1706.03762)

The pending tasks and resources for studying and understading the required concepts will be added soon.
